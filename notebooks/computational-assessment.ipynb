{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Feasibility Assessment  \n",
    "\n",
    "we would try to evaluates the dataset in terms of size, features, and overall feasibility for computational processing. Key aspects will include:  \n",
    "- **Dataset size**: Number of rows and columns  \n",
    "- **Feature types**: Categorical vs. numerical features  \n",
    "- **Memory usage**: Estimated RAM required to process the data  \n",
    "- **Missing values**: Percentage of missing data  \n",
    "- **Duplicate records**: Checking redundancy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the  train, test, and validation datasets\n",
    "base_path = \"../data/raw\"\n",
    "train_df = pd.read_csv(f\"{base_path}/hiv_train.csv\")\n",
    "test_df = pd.read_csv(f\"{base_path}/hiv_test.csv\")\n",
    "val_df = pd.read_csv(f\"{base_path}/hiv_valid.csv\")\n",
    "\n",
    "# Store datasets in a dictionary for easier processing\n",
    "datasets = {\"Train\": train_df, \"Test\": test_df, \"Validation\": val_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_dataset(name:str, df:pd.DataFrame) -> None:\n",
    "    print(f\"\\n===== {name} Dataset =====\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]:,} columns\")\n",
    "\n",
    "    # Memory usage in MB\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "    # Count categorical and numerical features\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    numerical_cols = df.select_dtypes(include=[\"int\", \"float\"]).columns\n",
    "    print(f\"Categorical Features: {len(categorical_cols)} | Numerical Features: {len(numerical_cols)}\")\n",
    "\n",
    "    # Missing values\n",
    "    missing_percentage = df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100\n",
    "    print(f\"Missing Data: {missing_percentage:.2f}%\")\n",
    "\n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Duplicate Records: {duplicates}\")\n",
    "\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Train Dataset =====\n",
      "Shape: 28,789 rows × 3 columns\n",
      "Memory Usage: 4.86 MB\n",
      "Categorical Features: 2 | Numerical Features: 1\n",
      "Missing Data: 0.00%\n",
      "Duplicate Records: 0\n",
      "----------------------------------------\n",
      "\n",
      "===== Test Dataset =====\n",
      "Shape: 8,225 rows × 3 columns\n",
      "Memory Usage: 1.39 MB\n",
      "Categorical Features: 2 | Numerical Features: 1\n",
      "Missing Data: 0.00%\n",
      "Duplicate Records: 0\n",
      "----------------------------------------\n",
      "\n",
      "===== Validation Dataset =====\n",
      "Shape: 4,113 rows × 3 columns\n",
      "Memory Usage: 0.69 MB\n",
      "Categorical Features: 2 | Numerical Features: 1\n",
      "Missing Data: 0.00%\n",
      "Duplicate Records: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run assessment for all datasets\n",
    "for name, df in datasets.items():\n",
    "    assess_dataset(name, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After evaluating the dataset based on size, feature types, memory usage, missing values, and duplicate records to determine its computational feasibility, I observed;\n",
    "\n",
    "- The dataset is small and well-structured, making local processing efficient.\n",
    "- Size: Train (28,789), Test (8,225), Validation (4,113) rows, each with 3 columns\n",
    "- Feature Types: 2 categorical, 1 numerical.\n",
    "- Memory Usage: Minimal (~6.94 MB in total), ensuring low computational overhead.\n",
    "- Missing Data: 0.00%, no need for imputation.\n",
    "- Duplicates: None, ensuring data integrity after the splits.\n",
    "\n",
    "Given the small size and low memory footprint, processing on a local machine is highly feasible without performance concerns. Although the Featurization stage requires loding and running anotheer ML model to create descriptors for each SMILES record in the dataset which might need more computational overhead, optimizing a script to preprocess the data with the ML model will be helpful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ersilia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
